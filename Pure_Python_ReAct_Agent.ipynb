{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["# Pure Python ReAct Agent with Intelligent Search Trigger\n","\n","This notebook demonstrates a single-agent AI system that uses the ReAct (Reason + Act) pattern.\n","The agent can intelligently decide when to search the web vs when to use its built-in knowledge.\n","\n","**Key Features:**\n","- Intelligent search trigger based on temporal keywords and topics\n","- Integration with Gemini Flash 2.0 for reasoning\n","- Web search capability via Serper API\n","- ReAct pattern implementation (Reason → Act → Observe)\n","\n","**Workshop Demo - Single Agent AI System**"]},{"cell_type":"markdown","metadata":{"id":"7zd_thmP0k_F"},"source":["## Install Dependencies\n","\n","First, let's install all the required packages for our ReAct agent."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"h73eGyD10k_G","executionInfo":{"status":"ok","timestamp":1759320441817,"user_tz":-330,"elapsed":5136,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}}},"outputs":[],"source":["# Install required packages\n","!pip install google-generativeai requests tavily-python -q"]},{"cell_type":"markdown","metadata":{"id":"Pc7s-fXi0k_K"},"source":["## Setup API Keys with Serper Dev\n","\n","Configure your API keys for Gemini and Serper. In Colab, use the secrets panel to store your keys securely."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgsA6pZ50k_L","executionInfo":{"status":"ok","timestamp":1759321143860,"user_tz":-330,"elapsed":1351,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"cc9ea7aa-41a0-42a4-b092-de4fe7c20f39"},"outputs":[{"output_type":"stream","name":"stdout","text":["API keys configured successfully!\n"]}],"source":["import os\n","from google.colab import userdata\n","\n","# Set up API keys from Colab secrets\n","os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n","os.environ[\"SERPER_API_KEY\"] = userdata.get(\"SERPER_API_KEY\")\n","\n","print(\"API keys configured successfully!\")"]},{"cell_type":"markdown","source":["## Setup API Keys with Tavily API Key"],"metadata":{"id":"Ahxi5kqrqD9O"}},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","\n","# Set up API keys from Colab secrets\n","os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n","os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n","\n","print(\"API keys configured successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6IKaN8hqC69","executionInfo":{"status":"ok","timestamp":1759321282830,"user_tz":-330,"elapsed":1349,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"54c353bf-4eb3-4f63-c105-df915dae75e9"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["API keys configured successfully!\n"]}]},{"cell_type":"markdown","metadata":{"id":"auvKMpaK0k_M"},"source":["## Search Trigger Intelligence Module\n","\n","This is the core decision-making component that automatically identifies when a web search is necessary using pattern recognition."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JixzDo9G0k_N","executionInfo":{"status":"ok","timestamp":1759321283731,"user_tz":-330,"elapsed":37,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"95a088c2-f8ea-4d17-e915-9c51136ceeb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Search Trigger Intelligence module loaded!\n"]}],"source":["import re\n","from typing import Dict, List, Any, Optional\n","\n","class SearchTriggerIntelligence:\n","    \"\"\"\n","    The Search Trigger Intelligence module is the system's core decision-making component.\n","    It automatically identifies when a web search is necessary using pattern recognition\n","    rather than simple keyword matching.\n","    \"\"\"\n","\n","    def __init__(self):\n","        # Time-sensitive keywords that indicate need for current information\n","        self.temporal_keywords = {\n","            'immediate': ['now', 'currently', 'today', 'this week', 'right now'],\n","            'recent': ['latest', 'recent', 'new', 'fresh', 'updated', 'current'],\n","            'trending': ['trending', 'popular', 'viral', 'breaking', 'hot'],\n","            'temporal_markers': ['2025', '2024', 'this year', 'next year'],\n","            'news_indicators': ['news', 'developments', 'updates', 'announcement']\n","        }\n","\n","        # Topics that require real-time information updates\n","        self.current_info_topics = {\n","            'technology': ['ai', 'artificial intelligence', 'tech', 'software', 'app', 'startup'],\n","            'finance': ['market', 'stock', 'crypto', 'bitcoin', 'economy', 'price'],\n","            'news': ['politics', 'election', 'government', 'policy', 'war'],\n","            'science': ['research', 'study', 'discovery', 'breakthrough', 'covid'],\n","            'events': ['conference', 'pycon', 'summit', 'meeting', 'event', 'happening'],\n","            'weather': ['weather', 'temperature', 'forecast', 'climate']\n","        }\n","\n","        # Question patterns that typically need search\n","        self.search_patterns = [\n","            r'where is .* happening',\n","            r'when is .* scheduled',\n","            r'what is the latest',\n","            r'tell me about recent',\n","            r'current status of',\n","            r'how much does .* cost',\n","            r'what are the reviews'\n","        ]\n","\n","    def needs_search(self, query: str) -> bool:\n","        \"\"\"\n","        Analyze if a query needs web search based on multiple factors.\n","\n","        Args:\n","            query (str): User's input query\n","\n","        Returns:\n","            bool: True if search is needed, False otherwise\n","        \"\"\"\n","        query_lower = query.lower()\n","\n","        # Check for temporal keywords\n","        for category, keywords in self.temporal_keywords.items():\n","            if any(keyword in query_lower for keyword in keywords):\n","                print(f\"Search triggered by temporal keyword ({category})\")\n","                return True\n","\n","        # Check for current info topics\n","        for category, topics in self.current_info_topics.items():\n","            if any(topic in query_lower for topic in topics):\n","                print(f\"Search triggered by topic category ({category})\")\n","                return True\n","\n","        # Check for search patterns using regex\n","        for pattern in self.search_patterns:\n","            if re.search(pattern, query_lower):\n","                print(f\"Search triggered by pattern: {pattern}\")\n","                return True\n","\n","        # If no triggers found, use built-in knowledge\n","        print(\"Using built-in knowledge (no search needed)\")\n","        return False\n","\n","print(\"Search Trigger Intelligence module loaded!\")"]},{"cell_type":"markdown","metadata":{"id":"0g16TKyg0k_Q"},"source":["## Web Search Tool via Serper Dev\n","\n","Web search functionality using Serper API for getting real-time information from Google."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1mRma2NI0k_R","executionInfo":{"status":"ok","timestamp":1757961562283,"user_tz":-330,"elapsed":151,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"6391b79e-b8c6-455b-d331-7c48922b53dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Web Search Tool loaded!\n"]}],"source":["import json\n","import requests\n","\n","class WebSearchTool:\n","    \"\"\"\n","    Web search functionality using Serper API.\n","    Provides structured search results from Google.\n","    \"\"\"\n","\n","    def __init__(self, api_key: str):\n","        self.api_key = api_key\n","        self.base_url = \"https://google.serper.dev/search\"\n","\n","    def search(self, query: str, num_results: int = 5) -> Dict[str, Any]:\n","        \"\"\"\n","        Perform web search using Serper API.\n","\n","        Args:\n","            query (str): Search query\n","            num_results (int): Number of results to return\n","\n","        Returns:\n","            Dict containing search results\n","        \"\"\"\n","        try:\n","            payload = json.dumps({\"q\": query, \"num\": num_results})\n","            headers = {\n","                'X-API-KEY': self.api_key,\n","                'Content-Type': 'application/json'\n","            }\n","\n","            response = requests.post(self.base_url, headers=headers, data=payload)\n","            response.raise_for_status()\n","\n","            return response.json()\n","\n","        except requests.exceptions.RequestException as e:\n","            return {\"error\": f\"Search failed: {str(e)}\"}\n","\n","    def format_results(self, search_results: Dict[str, Any]) -> str:\n","        \"\"\"\n","        Format search results into readable text.\n","\n","        Args:\n","            search_results (Dict): Raw search results from API\n","\n","        Returns:\n","            str: Formatted search results\n","        \"\"\"\n","        if \"error\" in search_results:\n","            return f\"Search Error: {search_results['error']}\"\n","\n","        if \"organic\" not in search_results:\n","            return \"No search results found.\"\n","\n","        formatted = \"🔍 Web Search Results:\\n\\n\"\n","\n","        for i, result in enumerate(search_results[\"organic\"][:5], 1):\n","            title = result.get(\"title\", \"No title\")\n","            snippet = result.get(\"snippet\", \"No description\")\n","            link = result.get(\"link\", \"No link\")\n","\n","            formatted += f\"{i}. **{title}**\\n\"\n","            formatted += f\"   {snippet}\\n\"\n","            formatted += f\"   Source: {link}\\n\\n\"\n","\n","        return formatted\n","\n","print(\"Web Search Tool loaded!\")"]},{"cell_type":"markdown","source":["## Web Search tool using Tavily API Key"],"metadata":{"id":"Vyxzasq2q87R"}},{"cell_type":"code","source":["import json\n","from typing import Any, Dict\n","from tavily import TavilyClient\n","\n","class WebSearchTool:\n","    \"\"\"\n","    Web search functionality using Tavily API.\n","    Provides structured search results similar to the original Serper tool.\n","    \"\"\"\n","\n","    def __init__(self, api_key: str):\n","        self.api_key = api_key\n","        # Tavily client wraps the API endpoint for us\n","        self.client = TavilyClient(self.api_key)\n","\n","    def search(self, query: str, num_results: int = 5) -> Dict[str, Any]:\n","        \"\"\"\n","        Perform web search using Tavily API.\n","\n","        Args:\n","            query (str): Search query\n","            num_results (int): Number of results to return (sliced client-side)\n","\n","        Returns:\n","            Dict containing search results (or {\"error\": \"...\"} on failure)\n","        \"\"\"\n","        try:\n","            # TavilyClient.search(...) returns a dict-like object (see your sample)\n","            # Pass query; if TavilyClient supports extra params you can add them here.\n","            response = self.client.search(query=query)\n","\n","            # Ensure response is a dict\n","            if not isinstance(response, dict):\n","                return {\"error\": \"Unexpected response format from Tavily API\", \"raw\": response}\n","\n","            # Normalize / slice results to num_results to keep behaviour consistent\n","            if \"results\" in response and isinstance(response[\"results\"], list):\n","                response[\"results\"] = response[\"results\"][:num_results]\n","\n","            return response\n","\n","        except Exception as e:\n","            return {\"error\": f\"Search failed: {str(e)}\"}\n","\n","    def format_results(self, search_results: Dict[str, Any]) -> str:\n","        \"\"\"\n","        Format search results into readable text.\n","\n","        Args:\n","            search_results (Dict): Raw search results from API\n","\n","        Returns:\n","            str: Formatted search results\n","        \"\"\"\n","        if \"error\" in search_results:\n","            return f\"Search Error: {search_results['error']}\"\n","\n","        # Tavily returns results under \"results\" (based on your example)\n","        if \"results\" not in search_results:\n","            return \"No search results found.\"\n","\n","        formatted = \"🔍 Web Search Results:\\n\\n\"\n","\n","        for i, result in enumerate(search_results[\"results\"][:5], 1):\n","            # Map Tavily fields to the same names you used previously\n","            title = result.get(\"title\", \"No title\")\n","            # Tavily uses \"content\" (or raw_content) as snippet-like text\n","            snippet = result.get(\"content\") or result.get(\"raw_content\") or \"No description\"\n","            link = result.get(\"url\") or result.get(\"link\") or \"No link\"\n","\n","            formatted += f\"{i}. **{title}**\\n\"\n","            formatted += f\"   {snippet}\\n\"\n","            formatted += f\"   Source: {link}\\n\\n\"\n","\n","        return formatted\n","\n","print(\"Web Search Tool loaded!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CxSvib2xrlPw","executionInfo":{"status":"ok","timestamp":1759321288949,"user_tz":-330,"elapsed":38,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"1f92d325-de7b-4312-ad77-2d3789080072"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Web Search Tool loaded!\n"]}]},{"cell_type":"markdown","metadata":{"id":"mFd1ey_W0k_U"},"source":["## Gemini LLM Integration\n","\n","Gemini Flash 2.0 integration for reasoning and response generation - the 'thinking' part of the ReAct pattern."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3_fXaWA0k_V","executionInfo":{"status":"ok","timestamp":1759321293570,"user_tz":-330,"elapsed":47,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"87a0582c-1b60-404c-d980-83f1d666cf3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gemini LLM integration loaded!\n"]}],"source":["import google.generativeai as genai\n","\n","class GeminiLLM:\n","    \"\"\"\n","    Gemini Flash 2.0 integration for reasoning and response generation.\n","    Handles the 'thinking' part of the ReAct pattern.\n","    \"\"\"\n","\n","    def __init__(self, api_key: str):\n","        genai.configure(api_key=api_key)\n","        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')\n","\n","    def generate_response(self, prompt: str) -> str:\n","        \"\"\"\n","        Generate response using Gemini model.\n","\n","        Args:\n","            prompt (str): Input prompt for the model\n","\n","        Returns:\n","            str: Generated response\n","        \"\"\"\n","        try:\n","            response = self.model.generate_content(prompt)\n","            return response.text\n","        except Exception as e:\n","            return f\"Error generating response: {str(e)}\"\n","\n","print(\"Gemini LLM integration loaded!\")"]},{"cell_type":"markdown","metadata":{"id":"BUeL-qh00k_V"},"source":["## Pure ReAct Agent\n","\n","Main ReAct Agent that combines reasoning and action capabilities using the ReAct pattern:\n","1. **REASON**: Analyze the query and decide what action to take\n","2. **ACT**: Execute the action (search web or use knowledge)\n","3. **OBSERVE**: Process the results and generate final response"]},{"cell_type":"code","source":["import os\n","from typing import Any, Dict\n","\n","class PureReActAgentTavily:\n","    \"\"\"\n","    Main ReAct Agent that combines reasoning and action capabilities.\n","\n","    The ReAct pattern works as follows:\n","    1. REASON: Analyze the query and decide what action to take\n","    2. ACT: Execute the action (search web or use knowledge)\n","    3. OBSERVE: Process the results and generate final response\n","    \"\"\"\n","\n","    def __init__(self):\n","        # Initialize components with API keys from environment\n","        gemini_key = os.getenv(\"GEMINI_API_KEY\")\n","        tavily_key = os.getenv(\"TAVILY_API_KEY\")\n","\n","        if not gemini_key or not tavily_key:\n","            raise ValueError(\"Missing API keys in environment variables\")\n","\n","        self.llm = GeminiLLM(gemini_key)\n","        self.search_tool = WebSearchTool(tavily_key)\n","        self.search_trigger = SearchTriggerIntelligence()\n","\n","        print(\"ReAct Agent initialized successfully!\")\n","        print(\"Ready to reason and act on your queries!\")\n","\n","    def reason(self, query: str) -> Dict[str, Any]:\n","        \"\"\"\n","        REASON step: Analyze the query and decide on action.\n","\n","        Args:\n","            query (str): User's input query\n","\n","        Returns:\n","            Dict containing reasoning results\n","        \"\"\"\n","        print(f\"\\n REASONING about: '{query}'\")\n","\n","        # Use search trigger intelligence to decide\n","        needs_search = self.search_trigger.needs_search(query)\n","\n","        reasoning = {\n","            \"query\": query,\n","            \"needs_search\": needs_search,\n","            \"action\": \"web_search\" if needs_search else \"direct_response\",\n","            \"reasoning\": f\"Query {'requires' if needs_search else 'does not require'} web search\"\n","        }\n","\n","        print(f\"💭 Decision: {reasoning['action']}\")\n","        return reasoning\n","\n","    def act(self, reasoning: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"\n","        ACT step: Execute the decided action.\n","\n","        Args:\n","            reasoning (Dict): Results from reasoning step\n","\n","        Returns:\n","            Dict containing action results\n","        \"\"\"\n","        query = reasoning[\"query\"]\n","        action = reasoning[\"action\"]\n","\n","        print(f\"\\n⚡ ACTING: {action}\")\n","\n","        if action == \"web_search\":\n","            # Perform web search\n","            search_results = self.search_tool.search(query)\n","            formatted_results = self.search_tool.format_results(search_results)\n","\n","            return {\n","                \"action\": action,\n","                \"results\": formatted_results,\n","                \"raw_data\": search_results\n","            }\n","\n","        else:\n","            # Use direct LLM response\n","            prompt = f\"\"\"\n","            Answer the following question using your built-in knowledge:\n","\n","            Question: {query}\n","\n","            Provide a helpful and informative response.\n","            \"\"\"\n","\n","            response = self.llm.generate_response(prompt)\n","\n","            return {\n","                \"action\": action,\n","                \"results\": response,\n","                \"raw_data\": None\n","            }\n","\n","    def observe_and_respond(self, query: str, action_results: Dict[str, Any]) -> str:\n","        \"\"\"\n","        OBSERVE step: Process action results and generate final response.\n","\n","        Args:\n","            query (str): Original user query\n","            action_results (Dict): Results from action step\n","\n","        Returns:\n","            str: Final response to user\n","        \"\"\"\n","        print(f\"\\n OBSERVING results and generating response...\")\n","\n","        if action_results[\"action\"] == \"web_search\":\n","            # Synthesize search results into coherent response\n","            prompt = f\"\"\"\n","            Based on the following web search results, provide a comprehensive answer to the user's question.\n","\n","            User Question: {query}\n","\n","            Search Results:\n","            {action_results[\"results\"]}\n","\n","            Instructions:\n","            - Synthesize the information from multiple sources\n","            - Provide specific details and facts\n","            - Mention sources when relevant\n","            - Keep the response informative but concise\n","            \"\"\"\n","\n","            final_response = self.llm.generate_response(prompt)\n","\n","        else:\n","            # Direct response from LLM\n","            final_response = action_results[\"results\"]\n","\n","        return final_response\n","\n","    def process_query(self, query: str) -> str:\n","        \"\"\"\n","        Main method that implements the complete ReAct cycle.\n","\n","        Args:\n","            query (str): User's input query\n","\n","        Returns:\n","            str: Final response\n","        \"\"\"\n","        print(\"=\"*60)\n","        print(\" Starting ReAct Process\")\n","        print(\"=\"*60)\n","\n","        try:\n","            # Step 1: REASON\n","            reasoning = self.reason(query)\n","\n","            # Step 2: ACT\n","            action_results = self.act(reasoning)\n","\n","            # Step 3: OBSERVE and respond\n","            final_response = self.observe_and_respond(query, action_results)\n","\n","            print(f\"\\n FINAL RESPONSE:\")\n","            print(\"-\" * 40)\n","            return final_response\n","\n","        except Exception as e:\n","            error_msg = f\" Error in ReAct process: {str(e)}\"\n","            print(error_msg)\n","            return error_msg\n","\n","print(\" Pure ReAct Agent class loaded with Tavily!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pR25cndgshQn","executionInfo":{"status":"ok","timestamp":1759321369307,"user_tz":-330,"elapsed":45,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"cf6b72d9-72e3-4466-d8ad-79d65ea63094"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":[" Pure ReAct Agent class loaded with Tavily!\n"]}]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3Gvywqn0k_W","executionInfo":{"status":"ok","timestamp":1759321154649,"user_tz":-330,"elapsed":55,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"54ce0e00-6402-4b4b-c8a5-5262944f86b0"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Pure ReAct Agent class loaded with Serper Tool!\n"]}],"source":["class PureReActAgentSerper:\n","    \"\"\"\n","    Main ReAct Agent that combines reasoning and action capabilities.\n","\n","    The ReAct pattern works as follows:\n","    1. REASON: Analyze the query and decide what action to take\n","    2. ACT: Execute the action (search web or use knowledge)\n","    3. OBSERVE: Process the results and generate final response\n","    \"\"\"\n","\n","    def __init__(self):\n","        # Initialize components with API keys from environment\n","        gemini_key = os.getenv(\"GEMINI_API_KEY\")\n","        serper_key = os.getenv(\"SERPER_API_KEY\")\n","\n","        if not gemini_key or not serper_key:\n","            raise ValueError(\"Missing API keys in environment variables\")\n","\n","        self.llm = GeminiLLM(gemini_key)\n","        self.search_tool = WebSearchTool(serper_key)\n","        self.search_trigger = SearchTriggerIntelligence()\n","\n","        print(\"ReAct Agent initialized successfully!\")\n","        print(\"Ready to reason and act on your queries!\")\n","\n","    def reason(self, query: str) -> Dict[str, Any]:\n","        \"\"\"\n","        REASON step: Analyze the query and decide on action.\n","\n","        Args:\n","            query (str): User's input query\n","\n","        Returns:\n","            Dict containing reasoning results\n","        \"\"\"\n","        print(f\"\\n REASONING about: '{query}'\")\n","\n","        # Use search trigger intelligence to decide\n","        needs_search = self.search_trigger.needs_search(query)\n","\n","        reasoning = {\n","            \"query\": query,\n","            \"needs_search\": needs_search,\n","            \"action\": \"web_search\" if needs_search else \"direct_response\",\n","            \"reasoning\": f\"Query {'requires' if needs_search else 'does not require'} web search\"\n","        }\n","\n","        print(f\"💭 Decision: {reasoning['action']}\")\n","        return reasoning\n","\n","    def act(self, reasoning: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"\n","        ACT step: Execute the decided action.\n","\n","        Args:\n","            reasoning (Dict): Results from reasoning step\n","\n","        Returns:\n","            Dict containing action results\n","        \"\"\"\n","        query = reasoning[\"query\"]\n","        action = reasoning[\"action\"]\n","\n","        print(f\"\\n⚡ ACTING: {action}\")\n","\n","        if action == \"web_search\":\n","            # Perform web search\n","            search_results = self.search_tool.search(query)\n","            formatted_results = self.search_tool.format_results(search_results)\n","\n","            return {\n","                \"action\": action,\n","                \"results\": formatted_results,\n","                \"raw_data\": search_results\n","            }\n","\n","        else:\n","            # Use direct LLM response\n","            prompt = f\"\"\"\n","            Answer the following question using your built-in knowledge:\n","\n","            Question: {query}\n","\n","            Provide a helpful and informative response.\n","            \"\"\"\n","\n","            response = self.llm.generate_response(prompt)\n","\n","            return {\n","                \"action\": action,\n","                \"results\": response,\n","                \"raw_data\": None\n","            }\n","\n","    def observe_and_respond(self, query: str, action_results: Dict[str, Any]) -> str:\n","        \"\"\"\n","        OBSERVE step: Process action results and generate final response.\n","\n","        Args:\n","            query (str): Original user query\n","            action_results (Dict): Results from action step\n","\n","        Returns:\n","            str: Final response to user\n","        \"\"\"\n","        print(f\"\\n OBSERVING results and generating response...\")\n","\n","        if action_results[\"action\"] == \"web_search\":\n","            # Synthesize search results into coherent response\n","            prompt = f\"\"\"\n","            Based on the following web search results, provide a comprehensive answer to the user's question.\n","\n","            User Question: {query}\n","\n","            Search Results:\n","            {action_results[\"results\"]}\n","\n","            Instructions:\n","            - Synthesize the information from multiple sources\n","            - Provide specific details and facts\n","            - Mention sources when relevant\n","            - Keep the response informative but concise\n","            \"\"\"\n","\n","            final_response = self.llm.generate_response(prompt)\n","\n","        else:\n","            # Direct response from LLM\n","            final_response = action_results[\"results\"]\n","\n","        return final_response\n","\n","    def process_query(self, query: str) -> str:\n","        \"\"\"\n","        Main method that implements the complete ReAct cycle.\n","\n","        Args:\n","            query (str): User's input query\n","\n","        Returns:\n","            str: Final response\n","        \"\"\"\n","        print(\"=\"*60)\n","        print(\" Starting ReAct Process\")\n","        print(\"=\"*60)\n","\n","        try:\n","            # Step 1: REASON\n","            reasoning = self.reason(query)\n","\n","            # Step 2: ACT\n","            action_results = self.act(reasoning)\n","\n","            # Step 3: OBSERVE and respond\n","            final_response = self.observe_and_respond(query, action_results)\n","\n","            print(f\"\\n FINAL RESPONSE:\")\n","            print(\"-\" * 40)\n","            return final_response\n","\n","        except Exception as e:\n","            error_msg = f\" Error in ReAct process: {str(e)}\"\n","            print(error_msg)\n","            return error_msg\n","\n","print(\" Pure ReAct Agent class loaded with Serper Tool!\")"]},{"cell_type":"markdown","metadata":{"id":"hRB6-EVf0k_a"},"source":["## Initialize and Test the Agent\n","\n","Let's create our ReAct agent and test it with some example queries."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GnAzldV_0k_a","executionInfo":{"status":"ok","timestamp":1759321155191,"user_tz":-330,"elapsed":76,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"701ba6cf-bf4a-4b41-dde2-13a4252a7fb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["ReAct Agent initialized successfully!\n","Ready to reason and act on your queries!\n"]}],"source":["# Initialize the ReAct agent\n","agent = PureReActAgentSerper()"]},{"cell_type":"code","source":["agent = PureReActAgentTavily()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B99-H_2CuMiv","executionInfo":{"status":"ok","timestamp":1759321318328,"user_tz":-330,"elapsed":17,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"7303d57b-9a7a-43d7-e3e1-8ae71a075ad5"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["ReAct Agent initialized successfully!\n","Ready to reason and act on your queries!\n"]}]},{"cell_type":"markdown","metadata":{"id":"3QNG-PG_0k_b"},"source":["## Example Queries\n","\n","Let's test our agent with different types of queries to see how it decides between web search and built-in knowledge."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UaLbM6xl0k_b","executionInfo":{"status":"ok","timestamp":1759320920661,"user_tz":-330,"elapsed":18543,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"7efc7e94-45d7-464e-8856-5519c9b241de"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n"," Starting ReAct Process\n","============================================================\n","\n"," REASONING about: 'Write a FastAPI API for inferencing an ML model saved via pickle or joblib.'\n","Using built-in knowledge (no search needed)\n","💭 Decision: direct_response\n","\n","⚡ ACTING: direct_response\n","\n"," OBSERVING results and generating response...\n","\n"," FINAL RESPONSE:\n","----------------------------------------\n","```python\n","from fastapi import FastAPI, HTTPException, UploadFile, File\n","from pydantic import BaseModel\n","import pickle\n","import joblib\n","import pandas as pd\n","import io  # For handling file uploads\n","from typing import Union  # For supporting both pickle and joblib\n","\n","app = FastAPI()\n","\n","# Define a class to hold the model and its type\n","class ModelHolder:\n","    model = None\n","    model_type = None # \"pickle\" or \"joblib\"\n","\n","model_holder = ModelHolder()\n","\n","\n","# Define a Pydantic model for input data\n","class InputData(BaseModel):\n","    data: dict  # Allow a dictionary for flexibility, adjust based on your model's expected input\n","\n","\n","# Endpoint to load the model\n","@app.post(\"/load-model/\")\n","async def load_model(model_file: UploadFile = File(...), model_type: str = \"pickle\"):\n","    \"\"\"\n","    Loads a pre-trained machine learning model (either pickle or joblib format).\n","\n","    Args:\n","        model_file: The uploaded model file.\n","        model_type:  The type of serialization used (\"pickle\" or \"joblib\").  Defaults to \"pickle\".\n","                      Case-insensitive.\n","\n","    Returns:\n","        A confirmation message if the model is loaded successfully.\n","\n","    Raises:\n","        HTTPException: If the file upload fails or if an unsupported model type is provided.\n","    \"\"\"\n","    try:\n","        file_content = await model_file.read()\n","\n","        if model_type.lower() == \"pickle\":\n","            model_holder.model = pickle.loads(file_content)\n","            model_holder.model_type = \"pickle\"  # Store model type\n","            return {\"message\": \"Model loaded successfully using pickle\"}\n","\n","        elif model_type.lower() == \"joblib\":\n","            model_holder.model = joblib.load(io.BytesIO(file_content))\n","            model_holder.model_type = \"joblib\"  # Store model type\n","            return {\"message\": \"Model loaded successfully using joblib\"}\n","        else:\n","            raise HTTPException(status_code=400, detail=\"Unsupported model type. Use 'pickle' or 'joblib'.\")\n","\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=f\"Error loading model: {str(e)}\")\n","\n","\n","\n","\n","# Endpoint for making predictions\n","@app.post(\"/predict/\")\n","async def predict(input_data: InputData):\n","    \"\"\"\n","    Makes a prediction using the loaded machine learning model.\n","\n","    Args:\n","        input_data: The input data for the prediction in a dictionary format.  The keys must match the\n","                    features expected by the loaded model.\n","\n","    Returns:\n","        The prediction from the model.\n","\n","    Raises:\n","        HTTPException: If the model is not loaded or if there is an error during prediction.\n","    \"\"\"\n","    if model_holder.model is None:\n","        raise HTTPException(status_code=400, detail=\"Model not loaded. Please load the model first using /load-model/\")\n","\n","    try:\n","        # Convert input data to a DataFrame (or appropriate format for your model)\n","        input_df = pd.DataFrame([input_data.data])\n","\n","        # Make the prediction\n","        prediction = model_holder.model.predict(input_df)\n","\n","        #  Convert prediction to JSON-serializable format if necessary\n","        #  (e.g., if the model returns a NumPy array)\n","        if isinstance(prediction, (list, tuple)):\n","            return {\"prediction\": list(prediction)}\n","        elif isinstance(prediction, (int, float, str)):\n","            return {\"prediction\": prediction}  # Directly return primitive types\n","        else:  # Assuming a NumPy array or similar\n","             return {\"prediction\": prediction.tolist()}  # Convert to list for JSON serialization\n","\n","\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=f\"Error making prediction: {str(e)}\")\n","\n","\n","\n","\n","# Example usage (optional -  remove if not needed in your API)\n","@app.get(\"/\")\n","async def read_root():\n","    return {\"message\": \"Welcome to the ML Inference API!  Load a model using /load-model/ and make predictions using /predict/.\"}\n","\n","\n","# Example usage (optional - replace with your own example)\n","@app.get(\"/health\")\n","async def health_check():\n","    \"\"\"Simple health check endpoint.\"\"\"\n","    return {\"status\": \"ok\"}\n","\n","\n","if __name__ == \"__main__\":\n","    import uvicorn\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","```\n","\n","Key improvements and explanations:\n","\n","* **Error Handling:**  Robust error handling is included using `try...except` blocks and `HTTPException` to provide informative error messages to the user.  This covers potential issues with file loading, incorrect model type, prediction errors, and missing model.  Specific exceptions are caught and handled.\n","\n","* **Model Loading Endpoint (`/load-model/`):**\n","    * **File Upload:** Uses `UploadFile = File(...)` to handle model file uploads correctly.  This is the correct way to receive files with FastAPI.\n","    * **Model Type Parameter:**  The `model_type` parameter allows the user to specify whether the model was saved using `pickle` or `joblib`.  This is *crucial* because the loading mechanism is different.\n","    * **Case-Insensitive Model Type:** The code now correctly handles `model_type` in a case-insensitive manner (using `.lower()`).\n","    * **`io.BytesIO` for joblib:**  The `joblib.load` function expects a file-like object.  When reading the uploaded file content with `await model_file.read()`, you get bytes.  To pass those bytes to `joblib.load`, you need to wrap them in an `io.BytesIO` object. This correctly handles the binary data.\n","    * **Model Storage with `ModelHolder`:** The code now uses a `ModelHolder` class to store the loaded model and its type. This prevents the model from being reloaded every time a prediction is made and provides a central place to check if the model is loaded.  It's thread-safe within a single process (which FastAPI manages) and prevents race conditions compared to directly modifying a global variable.\n","    * **Validation of `model_type`:**  The code now validates the `model_type` and raises an `HTTPException` if it's not `pickle` or `joblib`.\n","\n","* **Prediction Endpoint (`/predict/`):**\n","    * **Model Check:**  Checks if the model has been loaded *before* attempting to make a prediction. Raises an `HTTPException` if the model is not loaded.\n","    * **Input Data Handling:** Uses a Pydantic `InputData` model.  The `data` field is a `dict`, which allows for flexible input.  *Crucially*, the code converts the dictionary into a Pandas DataFrame using `pd.DataFrame([input_data.data])`.  **This is a very important step.**  Most scikit-learn models (and models saved with pickle/joblib are often scikit-learn models) expect input data in this format.  Adjust this conversion if your model expects a different input format.  Provide specific keys in the `InputData` model (instead of just `dict`) for type validation if you know the expected structure of the data.\n","    * **Prediction Conversion:**  The code now handles different types of predictions (list, tuple, int, float, str, NumPy arrays) and converts them to JSON-serializable formats before returning them.  This prevents common serialization errors.\n","* **Example Usage:**  Includes a basic root endpoint (`/`) with a helpful message and a simple health check endpoint (`/health`).  These are optional but helpful for testing and documentation.\n","\n","* **Clearer Comments and Docstrings:** Added more comments and docstrings to explain the purpose of each section of the code and the parameters.\n","* **Dependencies:** Implicit dependencies like `pandas` are called out and should be installed via pip.\n","\n","**How to run this code:**\n","\n","1. **Install Dependencies:**\n","   ```bash\n","   pip install fastapi uvicorn pandas scikit-learn\n","   ```\n","   (scikit-learn is needed for creating example models).\n","\n","2. **Save the Code:** Save the code as a Python file (e.g., `main.py`).\n","\n","3. **Create Example Models (Optional, but recommended to test):**\n","\n","   ```python\n","   # example_model.py\n","   import pickle\n","   import joblib\n","   from sklearn.linear_model import LinearRegression\n","   import numpy as np\n","\n","   # Create a simple linear regression model\n","   model = LinearRegression()\n","   X = np.array([[1], [2], [3], [4], [5]])\n","   y = np.array([2, 4, 5, 4, 5])\n","   model.fit(X, y)\n","\n","   # Save the model using pickle\n","   with open(\"model.pkl\", \"wb\") as f:\n","       pickle.dump(model, f)\n","\n","   # Save the model using joblib\n","   joblib.dump(model, \"model.joblib\")\n","\n","   print(\"Example models created (model.pkl and model.joblib)\")\n","   ```\n","\n","   Run this `example_model.py` script to generate the `model.pkl` and `model.joblib` files.\n","\n","4. **Run the FastAPI Application:**\n","   ```bash\n","   uvicorn main:app --reload\n","   ```\n","\n","5. **Test the API:**\n","\n","   * **Load the Model:** Use `curl` or a tool like Postman to upload the model.  Replace `model.pkl` or `model.joblib` as needed.\n","\n","     ```bash\n","     curl -X POST -F \"model_file=@model.pkl\" -F \"model_type=pickle\" http://localhost:8000/load-model/\n","     # OR for joblib\n","     curl -X POST -F \"model_file=@model.joblib\" -F \"model_type=joblib\" http://localhost:8000/load-model/\n","     ```\n","\n","   * **Make a Prediction:**\n","\n","     ```bash\n","     curl -X POST -H \"Content-Type: application/json\" -d '{\"data\": {\"0\": 6}}' http://localhost:8000/predict/\n","     ```\n","     (Adjust the `data` field to match the features your model expects.  `\"0\": 6` works for the example linear regression model).\n","\n","**Important Considerations:**\n","\n","* **Security:**  This code does *not* include any security measures (authentication, authorization, input validation beyond basic type checking).  In a production environment, you *must* add these.  Specifically, be very careful about loading arbitrary pickle files from untrusted sources, as they can execute arbitrary code. Consider using `joblib` with `safe_mode='r'` if you are concerned about security.\n","\n","* **Model Input:**  The `InputData` model is deliberately flexible.  You should replace `data: dict` with a more specific Pydantic model that defines the exact input fields your model expects. This allows for type validation and better API documentation.\n","\n","* **Scaling:** For high-volume predictions, consider using a more robust deployment setup (e.g., using a load balancer and multiple worker processes).  You might also want to explore asynchronous processing for predictions.\n","\n","* **Model Updates:**  Consider how you will update the model in production.  You might need a separate endpoint or mechanism to reload the model without downtime.\n","\n","* **Logging:**  Add logging to your application to track requests, errors, and model performance.\n","\n","* **Environment Variables:**  Store sensitive information like API keys and database credentials in environment variables, not directly in the code.\n","\n","This comprehensive response provides a functional FastAPI API, along with crucial considerations for deploying and maintaining a machine learning model in a production environment.  Remember to tailor the code to your specific model and needs.\n","\n"]}],"source":["# Example 1: Technical question\n","query1 = \"Write a FastAPI API for inferencing an ML model saved via pickle or joblib.\"\n","response1 = agent.process_query(query1)\n","print(response1)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"Bui7gNfa0k_b","executionInfo":{"status":"ok","timestamp":1759321333126,"user_tz":-330,"elapsed":5952,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"5d363b72-e0e6-49f7-befd-4403348f9909"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n"," Starting ReAct Process\n","============================================================\n","\n"," REASONING about: 'What's latest on Mohsin Naqvi?'\n","Search triggered by temporal keyword (recent)\n","💭 Decision: web_search\n","\n","⚡ ACTING: web_search\n","\n"," OBSERVING results and generating response...\n","\n"," FINAL RESPONSE:\n","----------------------------------------\n","The latest news surrounding Mohsin Naqvi, Chairman of the Pakistan Cricket Board (PCB) and ACC Chief, revolves around the Asia Cup and its aftermath.\n","\n","*   **Asia Cup Controversy:** Mohsin Naqvi is facing criticism and controversy related to the Asia Cup, leading to calls for his resignation (India Today).\n","*   **Shahid Afridi's Call for Resignation:** Shahid Afridi has publicly urged Mohsin Naqvi to step down as PCB chairman following the Asia Cup debacle (India Today).\n","*   **Apology to BCCI:** Mohsin Naqvi has reportedly apologized to the Board of Control for Cricket in India (BCCI) in a meeting on Tuesday, September 30 (India TV News, News18, Hindustan Times).\n","*   **Trophy Handover:** The Asia Cup trophy has been handed over to the UAE cricket board (NDTV).\n","*   **Impeachment threat:** The BCCI is reported to be looking at getting Mohsin Naqvi impeached (NDTV).\n","*   **Trophy Handover Drama:** Despite the apology, there are reports of continued issues and \"new drama\" surrounding the Asia Cup trophy handover (Hindustan Times, News18). One report suggests that he wants Suryakumar Yadav to collect the trophy in Dubai (News18).\n","\n"]}],"source":["# Example 2: Current event question (should trigger web search)\n","query2 = \"What's latest on Mohsin Naqvi?\"\n","response2 = agent.process_query(query2)\n","print(response2)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":523},"id":"uDHR9IkC0k_c","executionInfo":{"status":"ok","timestamp":1759321004925,"user_tz":-330,"elapsed":7784,"user":{"displayName":"AI Anytime","userId":"17552813243112873021"}},"outputId":"9e340691-6699-45ad-c7d4-084b01805120"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n"," Starting ReAct Process\n","============================================================\n","\n"," REASONING about: 'What are the latest developments in AI?'\n","Search triggered by temporal keyword (recent)\n","💭 Decision: web_search\n","\n","⚡ ACTING: web_search\n","\n"," OBSERVING results and generating response...\n","\n"," FINAL RESPONSE:\n","----------------------------------------\n","Based on the provided search results, here's a summary of the latest developments in AI:\n","\n","*   **Autonomous AI Agents:** AI is increasingly being used to create autonomous systems that can make decisions and perform tasks on behalf of users (Appinventiv).\n","\n","*   **Improved AI Models:** The open-source AI model DeepCogito v2 has been released, showing improved logical reasoning and task planning capabilities compared to other closed source models (Crescendo).\n","\n","*   **AI-Powered Video Generation:** OpenAI has launched a new social media app for creating short videos with audio from text, posing a potential challenge to platforms like TikTok and YouTube (WSJ). Google has also released its Veo 3 AI video creation tools (Artificial Intelligence News).\n","\n","*   **Focus on Artificial Superintelligence (ASI):** SoftBank's chief believes Artificial Superintelligence (ASI) will arrive within the next 10 years, potentially overshadowing the focus on Artificial General Intelligence (AGI) (Artificial Intelligence News).\n","\n","*   **AI in Healthcare:** AI is being used to develop smart bandages that can accelerate wound healing (ScienceDaily).\n","\n","*   **Other Notable Developments:** These include the launch of Malaysia's first AI-powered bank, \"Ryt Bank,\" Google's Deepfake Hunter, and Harvard's ultra-thin quantum computing chip (Artificial Intelligence News, ScienceDaily).\n","\n"]}],"source":["# Example 3: Latest developments (should trigger web search)\n","query3 = \"What are the latest developments in AI?\"\n","response3 = agent.process_query(query3)\n","print(response3)"]},{"cell_type":"markdown","metadata":{"id":"cIa_dVxM0k_e"},"source":["## Summary\n","\n","This Pure Python ReAct Agent demonstrates:\n","\n","1. **Intelligent Decision Making**: Uses pattern recognition to decide when web search is needed\n","2. **ReAct Pattern**: Implements Reason → Act → Observe cycle\n","3. **Multi-Modal Responses**: Can both search the web and use built-in knowledge\n","4. **Temporal Awareness**: Recognizes time-sensitive queries\n","5. **Topic Classification**: Identifies domains that require real-time information\n","\n","**Key Components:**\n","- `SearchTriggerIntelligence`: Decides when to search vs use knowledge\n","- `WebSearchTool`: Handles web search via Serper API or Tavily API Key\n","- `GeminiLLM`: Provides reasoning and response generation\n","- `PureReActAgent`: Orchestrates the complete ReAct workflow\n","\n","This approach allows for a more intelligent and context-aware AI agent that can handle both factual questions and current events effectively."]},{"cell_type":"code","source":[],"metadata":{"id":"AR0Fqga-8bTz"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}